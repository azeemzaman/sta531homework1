\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm}
\usepackage{graphicx}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\htheta}{\hat{\theta}}
%opening
\title{Sta 531 Homework 1}
\author{Azeem Zaman, NetID: azz2}

\begin{document}

\maketitle


\section{Exercise 1}
\subsection{Part a}
In this section we compute the first and second derivatives of the log posterior density.  First we must find the posterior.  Note that $p(\theta) = 1_{0,1}$ and $p(y_i|\theta) \propto 1/(1+(y_i-\theta)^2)$, so
\begin{align*}
 p(\theta|\by) &\propto p(\theta)\prod_{i=1}^5{p(y_i|\theta)} \\
 &\propto 1_{0,1}\prod_{i=1}^5{\frac{1}{1+(y_i-\theta)^2}}.
\end{align*}
Let $c$ by the normalizing constant and define $\lambda(\theta|\by)=\log{p(\theta|\by)}$. We have
\begin{align*}
 \lambda(\theta|\by) &= 1_{0,1}\left[\log{c}-\sum_{i=1}^5{\log(1 + (y_i - \theta)^2)}\right] \\
 \lambda'(\theta|\by) &= (2)1_{0,1}\left[\sum_{i=1}^5\frac{y_i - \theta}{1 + (y_i - \theta)^2}\right] \\
 \lambda''(\theta|\by) &= (2)1_{0,1}\left[\sum_{i=1}^5\frac{(y_i-\theta)^2-1}{\left[1+(y_i-\theta)^2\right]^2}\right].
\end{align*}
\subsection{Part b}
Note that the mode of a density $f(x)$ is the value $x$ that maximizes $f$.  As $\log$ is a monotonically increasing function, the value of $x$ that maximizes $f$ also maximizes $\log{f}$, which may be easier to maximize.  Furthermore, constants do not affect the value of $x$ that maximizes $f$, so we can maximize $p(\theta|\by)$ by finding a $\theta$ such that $\lambda'(\theta|\by) = 0$.  We are given $\by = (-2,-1,0,1.5,2.5)$, which we can use to find a suitable $\theta$ numerically.  Using the \texttt{uniroot} command in R, we find the posterior mode is $\hat{\theta} = -0.138$.  This command finds a value of $\theta$ such that $\lambda'(\theta|\by)=0$ on a specified interval (we guessed $\htheta \in [-1,1]$).  We find $\lambda''(\hat{\theta}|\by)=-1.385$, so this is indeed a maximum.  

Computing the maximum a posteriori (MAP), which we shall denote $\theta_M$ is now simple.  The MAP cannot coincide with the M.L.E. because the support of the posterior is $[0,1]$ and $\htheta <0$.  A plot of the derivative of the posterior, $\lambda'(\theta|\by)$, is given in Figrure \ref{mapplot}.  As the derivative is negative across the whole support, we conclude that $\theta_M = 0$.  

\begin{figure}
 \centering
 \includegraphics[width = .75\textwidth]{mapplot.pdf}
 \caption{\label{mapplot} Plot of the derivative of the logarithm of the posterior denisty.  Since the prior is uniform, this is also the score function plotted on the support of the posterior.}
\end{figure}


\subsection{Part c}
The normal approximation is
\begin{align*}
 p(\theta|\by) \approx N(\hat{\theta}, [I(\htheta)]^{-1}),
\end{align*}
where $\hat{\theta}$ is either the posterior mode or the M.L.E (both found above) and $I(\htheta)$ is the observed information, which in the univariate case is
\begin{align*}
 I(\theta) = -\lambda''(\theta|\by).
\end{align*}

\begin{figure}
\centering
\includegraphics[width = .75\textwidth]{normapprox.pdf}
\caption{\label{mleapprox} Normal approximation using the M.L.E for $\htheta$.}
\end{figure}
If we use the M.L.E. for $\htheta$ we find $\mu = -0.138$ and $\sigma^2 = 0.727$. This approximation is shown in Figure \ref{mleapprox}.

\begin{figure}
 \centering
 \includegraphics[width = .75\textwidth]{mapapprox.pdf}
 \caption{\label{mapapprox} The normal approximation using the MAP}
\end{figure}
We can also use the MAP for $\htheta$.  In this case we have $\htheta = 0$ and $\sigma^2 = -(\lambda''(0|\by))^{-1} = 0.756$.  This approximation is shown in Figure \ref{mapapprox}.

\section{Exercise 4}
\subsection{Proof that $p(\phi|\bx)$ is asymptotically normal}
The proof of this fact relies on the delta method. From lecture we know that $p(\theta|\bx) \overset{D}{\rightarrow} N(\hat{\theta}, I(\hat{\theta};\bx)^{-1})$, where $\hat{\theta}$ is the posterior mode.  Let $\sigma^{2}=[I(\hat{\theta};\bx)/n]^{-1}$, so we can write $\theta_{n} \overset{A}{\sim} N(\hat{\theta}, \sigma^{2}/n)$, where $\theta_{n}$ is a random variable distributed as $p(\theta|\bx)$ and $n$ is the length of the vector of observations, $\bx$.   The continuous mapping theorem states allows us to conclude that $\sqrt{n}(\theta_{n}-\hat{\theta}) \overset{D}{\rightarrow} N(0, \sigma^{2})$.  As we are assuming that $\phi$ is one-to-one, continuous, and smooth, we can in particular conclude that the first derivative exists and is not zero (otherwise the function would not be one-to-one).  This allows us to apply the Delta method, which tells us that $\sqrt{n}(\phi(\theta_{n})-\phi(\hat{\theta})) \overset{D}{\rightarrow} N(0, \sigma^{2}\phi'(\hat{\theta})^{2})$.  Applying the continuous mapping theorem again reveals that
\begin{align*}
\phi(\theta_{n}) \overset{A}{\sim} N(\phi(\hat{\theta}), \sigma^{2}\phi'(\hat{\theta})^{2}n^{-1}).
\end{align*}
Therefore the random variable $\phi(\theta_{n})$ is asymptotically normal with mean $\phi(\hat{\theta})$ and variance $I(\hat{\theta};\bx)\phi(\hat{\theta})^{2}$.  

\subsection{Intuition for the result}
If $\phi$ were a linear function, the result would be clear.  As we are not requiring $\phi$ to be linear, and non-linear transformations of normally distributed random variables are no longer normal, the result is somewhat surprising.  The function $\phi$ satisfies the conditions of the mean value theorem, so we know that there exists a point $\theta_{0}$ such that
\begin{align*}
\frac{\phi(\theta_{n})-\phi(\hat{\theta})}{\theta_{n} - \hat{\theta}} &= \phi'(\theta_{0}) \\
\phi(\theta_{n})-\phi(\hat{\theta}) &= \phi'(\theta_{0})\left(\theta_{n} - \hat{\theta}\right) \\
\phi(\theta_{n}) &= \phi(\hat{\theta}) + \phi'(\theta_{0})\left(\theta_{n} - \hat{\theta}\right). \\
\end{align*}
So the mean value shows that there exists a linear approximation for $\phi$.  We can rewrite this as
\begin{align*}
\sqrt{n}\left(\phi(\theta_{n})-\phi(\hat{\theta})\right) &= \sqrt{n}\phi'(\theta_{0})\left(\theta_{n} - \hat{\theta}\right).
\end{align*}
The problem here is the point $\theta_{0}$, which can take any value between $\theta_{n}$ and $\hat{\theta}$; as $n$ increases the value of $\theta_{0}$ could change erratically.  This turns out not to be a problem because $\theta_{n} \to \hat{\theta}$ as $n \to \infty$, which forces $\theta_{0} \to \hat{\theta}$.  So we have
\begin{align*}
\sqrt{n}\left(\phi(\theta_{n})-\phi(\hat{\theta})\right) &= \sqrt{n}\phi'(\theta_{0})\left(\theta_{n} - \hat{\theta}\right) \\
&\to  \sqrt{n}\phi'(\hat{\theta})\left(\theta_{n} - \hat{\theta}\right) \\
&\to N(0, \sigma^{2}\phi'(\hat{\theta})^{2})
\end{align*}
as $n \to \infty$ (we know from other results that $\sqrt{n}\left(\theta_{n} - \hat{\theta}\right) \to N(0, \sigma^{2})$).  The result can thus be understood intuitively as follows:  even if $\phi$ is non-linear, as $n \to \infty$ a good linear approximation will appear, which can be used to show that the limiting distribution should also be normal.  

\subsection{Empirical Demonstration}
Recall that $\theta_{0}$ is a value that satisfying
\begin{align*}
\frac{\phi(\theta_{n})-\phi(\hat{\theta})}{\theta_{n} - \hat{\theta}} &= \phi'(\theta_{0}),
\end{align*}
which must exist by the mean value theorem.  We have argued that when $n$ is large we can get a linear approximation for $\phi$ because $\phi'(\theta_{0}) \approx \phi'(\hat{\theta})$.  To see this concretely, let $\phi(\theta_{n})=\theta_{n}^{2}$ and consider the model
\begin{align*}
\theta &\sim Exp(1) \\
X_{1}, \ldots, X_{n}|\theta &\sim Exp(\theta).
\end{align*}
This results in the posterior $Gamma(1+n,1+\sum_{i=1}^{n}x_{i})$.  Suppose the true value of $\theta$ is 1, so we are sampling from $Exp(1)$.  For various $n$ we can sample from this distribution and calculate a posterior.  We can then sample, for example, 1000 values for $\theta_{n}$ from these posteriors.  For each of these $\theta_{n}$ we can compute $\theta_{0}$:
\begin{align*}
\phi'(\theta_{0}) &=  \frac{\phi(\theta_{n})-\phi(\hat{\theta})}{\theta_{n} - \hat{\theta}} \\
2\theta_{0} &= \frac{\theta_{n}^{2}-1}{\theta_{n}-1} \\
\theta_{0} &= \frac{\theta_{n}+1}{2}.
\end{align*}
For our approximation to be good, we need $\theta_{0} \approx 1$.  By averaging the values for $\theta_{0}$ we can get an idea how good our approximation is at various values of $n$.  The plot in Figure \ref{meanval} shows that we indeed have $\theta_{0} \approx 1$ when $n$ is even moderately large.
\begin{figure}
\centering
\includegraphics[width = .75\textwidth]{meanval.pdf}
\caption{\label{meanval} The average $\theta_{0}$ for 1000 draws of $\theta_{n}$ at various $n$}
\end{figure}

\section{Exercise 9}
\subsection{The restricted M.L.E.}
Let $f(y|\theta)$ be likelihood function and $\lambda(y|\theta)$ be the log likelihood.  We have
\begin{align*}
\lambda(y|\theta) &= \log{c} - \frac{1}{2\sigma^{2}}(y - \theta)^{2}, \\
\lambda'(y|\theta) &= \frac{1}{\sigma^{2}}(y - \theta), \\
\lambda'(y|\theta) &= -\frac{1}{\sigma^{2}}.
\end{align*}
This means that $\htheta = y$ is the M.L.E.  We are restricting to the range $[0,1]$.  If $y<0$, then $\lambda'(y|\theta)$ is negative in $\theta$, so want to pick the smallest possible value of $\theta$, which is $\htheta = 0$.  Conversely, if $y > 1$, then $\lambda'(y|\theta)$ is positive on $[0,1]$, so we want to pick the largest possible value of $\theta$, which is $\htheta = 1$.  Therefore the M.L.E. is
\begin{align*}
\htheta(y) &= \begin{cases}
y & y \in [0,1] \\
0 & y < 0 \\
1 & y > 1. \end{cases}
\end{align*}
\subsection{The posterior mean}
The posterior $p(\theta|y)$ satisfies
\begin{align*}
p(\theta|y) &\propto p(y|\theta)p(\theta) \\
&\propto 1_{0,1}\exp\left(-\frac{(y-\theta)^{2}}{2\sigma^{2}}\right).
\end{align*}
This is the kernel of a truncated normal, so $\theta|y \sim N(y,\sigma^2, 0, 1)$, where $0$ and $1$ represent the truncation points for the normal distribution.  The mean of this distribution is
\begin{align}\label{truncmean}
 E(\theta|y,\sigma^2,0,1) = y + \sigma\left(\frac{\phi\left(\frac{-y}{\sigma}\right)-\phi\left(\frac{1-y}{\sigma}\right)}{\Phi\left(\frac{1-y}{\sigma}\right)-\Phi\left(\frac{-y}{\sigma}\right)}\right).
\end{align}
\subsection{Computing the MSE}
Computing the expectation for the M.L.E. (needed to compute the bias) is possible.  If $\theta_0$ is the true value of $\theta$, we have
\begin{align*}
 E(\htheta|y) &= \int_{-\infty}^\infty \htheta(y)f(y|\theta_0, \sigma^2) \, dy \\
 &= \int_0^1 yf(y|\theta_0, \sigma^2) \, dy + \int_1^\infty f(y|\theta_0, \sigma^2) \, dy.
\end{align*}
This can be evaluated without too much difficulty.  The expectation for the posterior mean is much harder; the expectation of the second term in \ref{truncmean} is much harder.  Thankfully, we do not have to evlaute the expectation.  Instead, we can pick a true value $\theta_0$ and simulate draws for $y$.  With these draws we can compute the sample mean, which is an unbiased estimator of the expectation.  Furthermore, we can compute an unbiased sample variance.  With these quantities (sample mean, sample variance, and $\theta_0$), we can estimate the MSE.   The plots in Figure \ref{mseplots} show that by the time the variance is equal to 1, the MSE for the posterior mean is lower than the MSE for the M.L.E. estimator.  

\begin{figure}
 \centering
 \includegraphics[width = .5\textwidth]{mse0.pdf}
 \includegraphics[width = .5\textwidth]{mse5.pdf}
 \includegraphics[width = .5\textwidth]{mse1.pdf}
 \caption{\label{mseplots} Comparison of the MSE for the M.L.E. estimator and the posterior mean at various variances.}
\end{figure}


\section{Exercise 13}
\subsection{Weak, uniform, or improper priors}
Even if one is using a non-informative prior, there are still some advantages to Bayesian approaches.  For example, Bayesian results are often intuitevly appealing.  Frequentist confidence intervals are notoriously difficult to interpret, and many people misinterpret these intervals.  Bayesian credible intervals have a much more intuitve interpretation.  If $\theta$ is some unknown parameter, we can construct some region $A$ such that the probability that $\theta \in A$ is 95\%.  In a Frequentist setting we construct some random interval with the property that if we computed this interval many times we would expect 95\% of the constructed intervals to contain the true value $\theta$.  Once we construct this interval, however, it either contains the true $\theta$ or it does not (and we do not know whether it is contained).  If the results of our analysis are going to be shared with non-statisticians, it may be simpler for them to understand.

Bayesian credible intervals have another advantage:  they only give positive probability to parameter values in the parameter space.  A confidence interval, conversely, can include impossible values.  This is not just a theoretical concern, it commonly happens if the unknown parameter is a Bernoulli probability $p$ that is close to 0 or 1.  

\subsection{A strong prior}
Ehrenberg's objection to a strong prior (``why collect more data?'') is rather absurd. Even if you believe you already have a good model, why \emph{not} collect more data?  New data could be used to check an existing model.  In addition, it is easy to update a prior to reflect new information, so more data further improves a model.

A better objection might be: can you ever justify a strong prior?  Is there ever a situation where one can justify the use of a strong prior?  There probably is such a situation, but they are rare.  Any Bayesian will tell you that picking a prior is hard.  

\subsection{The in between cases}
The main strength of Bayesian statistics is the ability to incorporate prior beliefs into a model.  Most of the time our prior beliefs are very general and cannot be used to create a ``strong'' prior.  Instead, we select a prior that has some shape (i.e. not uniform) and attempts to reflect our beliefs.  

\section{Exercise 15}
\subsection{Part a}
Suppose $c(\gamma)$ is a 50\% credible region in the posterior.  Let $1_{c(\gamma)}$ be the indicator function for this region.  We have $E[1_{c(\gamma)}|y]=0.5$, so the Law of Total Expecation (Iterated Expecation) says
\begin{align*}
0.5 &= E(0.5) \\
&= E\left(E[1_{c(\gamma)}|y]\right) \\
&= E[1_{c(\gamma)}].
\end{align*}
In short, $E(1_{c(\gamma)})=0.5$, which means that the probability that $\theta \in c(\gamma)$ is 50\%.  Note that this probability is no longer conditional on $y$.  So since we are drawing a true $\theta_{0}$ from the prior, and there is a 50\% chance that $\theta$ drawn from the prior is in the set $c(\gamma)$, there is a 50\% chance that the true that $\theta_{0}$ will be contained in the set $c(\gamma)$.  

\subsection{Part b}
The posterior satisfies
\begin{align*}
p(\theta|y) &\propto p(\theta)p(y|\theta) \\
&\propto \exp\left(-\frac{1}{2(4)}\theta^{2}\right)\exp\left(-\frac{1}{2}(y - \theta)^{2}\right) \\
&\propto \exp\left(-\frac{1}{2(4)}\theta^{2}-\frac{1}{2}\left(y^{2}-2y\theta + \theta^{2}\right)\right) \\
&\propto \exp\left(-\frac{1}{2}\left((5/4)\theta^{2}-2y\theta\right)\right) \\
&\propto \exp\left(-\frac{1}{2(4/5)}\left(\theta^{2}-2(4/5)y\theta\right)\right) \\
&\propto \exp\left(-\frac{1}{2(4/5)}\left(\theta^{2}-2(4/5)y\theta + ((4/5)^{2}y^{2} - (4/5)^{2}y^{2)}\right)\right) \\
&\propto \exp\left(-\frac{1}{2(4/5)}\left(\theta^{2}-2(4/5)y\theta + (4/5)^{2}y^{2}\right)\right) \\
&\propto \exp\left(-\frac{1}{2(4/5)}\left(\theta - (4/5)y\right)^{2}\right), \\
\end{align*}
which is the kernel of a normal distribution with mean $4y/5$ and variance $4/5$, so $\theta|y \sim N(4y/5, 4/5)$.  If we generically let $\mu(y) = 4y/5$ and $\sigma = \sqrt{4/5}$ then $(\theta-\mu(y))/\sigma$ has a standard normal distribution (conditional on $y$).  So if we want to construct a $\gamma$\% posterior interval, let $c(\gamma) = \Phi((1+\gamma)/2)$.  This gives
\begin{align*}
P\left(\frac{\theta-\mu(y)}{\sigma} > c(\gamma)|y\right) &= \frac{1-\gamma}{2} \\
P(\left(\mu(y) + c(\gamma)\sigma < \theta|y\right) &= \frac{1-\gamma}{2}.
\end{align*}
Using the symmetry of the normal we can conclude that $C(y, \gamma) = (\mu(y)-c(\gamma)\sigma,\mu(y)+c(\gamma)\sigma)$ is a $\gamma$\% confidence interval.  Suppose $\theta_{0}$ is the true population mean, so $Y \sim N(\theta_{0},1)$.  We can calculate the probability that $\theta_{0} \in C(y, \gamma)$.  To do this, it is easier to calculate the probability that $\theta_{0} \notin c(\gamma)$.  This can occur in two ways.  First, we could have $\theta_{0} < \mu(y)-c(\gamma)\sigma$, which occurs with probability 
\begin{align*}
p_{1} &= P(\theta_{0}<\mu(y)-c(\gamma)\sigma) \\
&= P(\theta_{0}<4y/5-c(\gamma)\sigma) \\
&= P\left(\frac{5(\theta_{0}+c(\gamma)\sigma)}{4} < y\right).
\end{align*}
As we know that $Y \sim N(\theta_{0},1)$, we can compute this probability exactly ($\theta_{0}, c(\gamma),$ and $\sigma$ are all known).  We also need to compute the probability that $\theta_{0} > \mu(y) + c(\gamma)\sigma$ (our interval underestimates the true value).  This is
\begin{align*}
p_{2} &= P(\theta_{0}>\mu(y)+c(\gamma)\sigma) \\
&= P(\theta_{0}>4y/5-c(\gamma)\sigma) \\
&= P\left(\frac{5(\theta_{0}-c(\gamma)\sigma)}{4} > y\right).
\end{align*}
The coverage is then $1 - (p_{1}+p_{2})$.  We have $\theta_{0}=1$ and $\gamma = .5$, so we can compute the coverage to be 63.9\%.  

\subsection{Part c}
Using the framework outlined above, we can calculate the coverage for a range of $\theta_{0}$ values.  We can also plot the frequentist coverage, which is always the specified value (50\%).  This is presented in Figure \ref{cover}.  
\begin{figure}
\centering
\includegraphics[width = \textwidth]{coverage.pdf}
\caption{\label{cover} A comparison of Bayesian 50\% interval coverage and Frequentist 50\% interval coverage.}
\end{figure}
Note that for values of $\theta_{0}$ that are close to the prior mean ($\theta_{0}=0$), the Bayesian coverage is higher than Frequentist coverage.  As $\theta_{0}$ moves away from the prior mean, the Bayesian coverage rapidly approaches zero.  We see that if our prior beliefs are roughly correct, Bayesian coverage will be superior to frequentist coverage.  
\end{document}
